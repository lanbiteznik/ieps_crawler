{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfd0b4f-f9f3-4048-bf78-ea9db29eb016",
   "metadata": {},
   "source": [
    "# Preferential crawling\n",
    "Preferential crawling is a web crawling strategy where a crawler **prioritizes certain web pages** based on predefined criteria instead of visiting all links indiscriminately. Unlike a standard breadth-first or depth-first crawler, which processes URLs in the order they are discovered, a preferential crawler gives higher priority to pages that match specific characteristics.  \n",
    "\n",
    "<img src=\"https://i.ibb.co/sp2ZG5BZ/Posnetek-zaslona-2025-03-03-ob-12-45-11.png\" alt=\"Posnetek-zaslona-2025-03-03-ob-12-45-11\" border=\"0\">\n",
    "\n",
    "> Image from the paper Shrivastava et al.: A Study of Focused Web Crawling Techniques\n",
    "\n",
    "### **How Preferential Crawling Works**  \n",
    "\n",
    "1. **Seed URLs** – The crawler starts with an initial set of URLs to visit.  \n",
    "2. **Fetching & Parsing** – It retrieves web pages and extracts links.  \n",
    "3. **Prioritization** – Links are **ranked and queued** based on a preference mechanism, such as:  \n",
    "   - **Keyword Matching** – Pages containing certain keywords in their URL, metadata, or content.  \n",
    "   - **Domain Preferences** – Prioritizing links from a specific domain or set of domains.  \n",
    "   - **Page Importance** – Using metrics like PageRank, backlinks, or update frequency.  \n",
    "   - **Topic Relevance** – Filtering pages using Natural Language Processing (NLP) or machine learning.  \n",
    "4. **Crawling Execution** – The crawler processes the prioritized URLs first before moving to lower-priority links.  \n",
    "5. **Loop & Termination** – The process continues until a predefined condition is met (e.g., max pages reached, time limit, or no new prioritized links).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70643575-5c9e-45f0-830b-cb256472a252",
   "metadata": {},
   "source": [
    "## A simple example of a preferential webcrawler\n",
    "This example works by defining a keyword we are interested in and aranging the links in the queue based on how well they match the target word. For now we are only checking whether the link tag contains the keyword and assigning hign priprity to links with the given keyword and low priority to others.\n",
    "\n",
    "In this exercise we will adapt this crawler to use more sophisticated techniques for ranking the links in its queue.\n",
    "\n",
    "> Does this code recalculate priorities when a link is rediscovered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff6007-7045-49c9-9692-3b37772488b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6668d-ad76-47b4-913a-09c533ca2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferentialWebCrawler:\n",
    "    def __init__(self, seed_url, keyword, max_pages=10):\n",
    "        self.seed_url = seed_url\n",
    "        self.keyword = keyword\n",
    "        self.max_pages = max_pages\n",
    "        url_parts = urlsplit(seed_url)\n",
    "        self.domain = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "        self.visited = set()\n",
    "        self.queue = []  # Priority queue (min-heap)\n",
    "        heapq.heappush(self.queue, (0, seed_url))  # Start with the seed URL (highest priority)\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Fetch page content from a URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def extract_links(self, html, base_url):\n",
    "        \"\"\"Extract and return all links from the HTML content.\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = []\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag[\"href\"]\n",
    "            if href.startswith(\"http\"):\n",
    "                links.append((href, a_tag))\n",
    "            elif href.startswith(\"//\"):\n",
    "                links.append((\"https:\" + href, a_tag))\n",
    "            elif href.startswith(\"/\"):\n",
    "                links.append((base_url + href, a_tag))\n",
    "        return links\n",
    "\n",
    "    def in_domain(self, url):\n",
    "        return url.startswith(self.domain)\n",
    "\n",
    "    def priority(self, html, link, link_tag):\n",
    "        \"\"\"\n",
    "        Compute the priority of a link.\n",
    "\n",
    "        Args:\n",
    "            html (str): HTML content of the page.\n",
    "            link (str): Link URL.\n",
    "            link_tag (bs4.Tag): BeautifulSoup tag representing the link.\n",
    "\n",
    "        Returns:\n",
    "            float: Priority score (lower number represents high priority).\n",
    "        \"\"\"\n",
    "        priority = 0 if self.keyword in link else 1  # Assign priority (0 = high, 1 = lower)\n",
    "        return priority\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Crawl pages, prioritizing links containing the keyword.\"\"\"\n",
    "        pages_crawled = 0\n",
    "\n",
    "        while self.queue and pages_crawled < self.max_pages:\n",
    "            priority, url = heapq.heappop(self.queue)  # Get the highest-priority URL\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "            \n",
    "            if not self.in_domain(url):\n",
    "                continue\n",
    "\n",
    "            print(f\"Crawling (Priority {priority}): {url}\")\n",
    "            html = self.fetch_page(url)\n",
    "            if not html:\n",
    "                print(\"  - Not found\")\n",
    "                continue\n",
    "\n",
    "            self.visited.add(url)\n",
    "            pages_crawled += 1\n",
    "\n",
    "            url_parts = urlsplit(url)\n",
    "            base_url = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "            links = self.extract_links(html, base_url)\n",
    "            print(\"  - Found\", len(links), \"links\")\n",
    "            for link, link_tag in links:\n",
    "                if link not in self.visited:\n",
    "                    priority = self.priority(html, link, link_tag)\n",
    "                    heapq.heappush(self.queue, (priority, link))\n",
    "\n",
    "\n",
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"Nobel\"  # Prioritize links containing this keyword\n",
    "crawler = PreferentialWebCrawler(seed, keyword, max_pages=5)\n",
    "crawler.crawl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f41070",
   "metadata": {},
   "source": [
    "### **Improving the Priority Function**  \n",
    "\n",
    "In this example, we are only checking if the URL link includes a keyword to determine its priority. There are many situations in which this approach is not good enough. A URL alone might not fully represent the relevance or quality of the content. For example, important information might be hidden in the page text, metadata, or title, rather than in the URL itself. Additionally, some high-quality pages might not contain the keyword in the URL but still offer highly relevant content.  \n",
    "\n",
    "A more advanced priority function could consider additional factors such as:  \n",
    "\n",
    "- **Page Content**: Checking if the keyword appears in the text or headings.  \n",
    "- **Meta Tags**: Analyzing meta descriptions and titles for keyword matches.  \n",
    "- **Link Popularity**: Prioritizing pages with more inbound links.  \n",
    "- **URL Depth**: Giving higher priority to links closer to the seed URL.  \n",
    "- **Page Freshness**: Favoring recently updated content.  \n",
    "\n",
    "By combining multiple factors, the crawler can make better decisions about which pages to visit first, improving both efficiency and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of unsuccessfull crawl\n",
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"president\"  # Our goal was to see societies that einstein was the presitent of\n",
    "crawler = PreferentialWebCrawler(seed, keyword, max_pages=5)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c389e",
   "metadata": {},
   "source": [
    "### **Checking Surrounding Text**  \n",
    "\n",
    "When determining the link priority, we cannot use the content of the target website, as it has not been downloaded yet. Because of that, we will use only the information already available on the current website.\n",
    "\n",
    "The most common way to compute the priority more accurately is to consider the text around the link on the webpage. The surrounding text often provides important context about the linked page, helping to estimate its relevance.\n",
    "\n",
    "By analyzing this context, the crawler can assign higher priority to links with surrounding text that matches predefined keywords or topics. This method improves precision, especially when URLs alone do not reveal the nature of the content. Additionally, weighting the importance of anchor text or heading tags (such as `<h1>` or `<h2>`) can further refine the priority calculation.  \n",
    "\n",
    "> In the next example, we will observe link neighbourhood and check if it contains the target keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd650ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferentialWebCrawler2(PreferentialWebCrawler):\n",
    "    def priority(self, html, link, link_tag):\n",
    "        \"\"\"\n",
    "        Compute the priority of a link.\n",
    "\n",
    "        Args:\n",
    "            html (str): HTML content of the page.\n",
    "            link (str): Link URL.\n",
    "            link_tag (bs4.Tag): BeautifulSoup tag representing the link.\n",
    "\n",
    "        Returns:\n",
    "            float: Priority score (lower number represents high priority).\n",
    "        \"\"\"\n",
    "        window_size = 50\n",
    "        # Get the content of the parent tag to the link\n",
    "        sourounding_text = link_tag.parent.text\n",
    "        index = sourounding_text.find(link_tag.text)\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(sourounding_text), index + window_size)\n",
    "        sourounding_text = sourounding_text[start:end]\n",
    "        \n",
    "        priority = 0 if self.keyword.lower() in sourounding_text.lower() else 1  # Assign priority (0 = high, 1 = lower)\n",
    "        return priority\n",
    "\n",
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"Nobel\"  # Prioritize links containing this keyword\n",
    "crawler = PreferentialWebCrawler2(seed, keyword, max_pages=5)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a11d59",
   "metadata": {},
   "source": [
    "We see that we get completely irrelevant pages (worse than before). This happenes because all links that are somewhat close to the target keyword now have high priority. For example in the following paragraph all links would have high priority even if they do not talk directly about the nobel prize.\n",
    "> <img src=\"https://i.ibb.co/TB3rdCSn/Posnetek-zaslona-2025-03-03-ob-10-36-24.png\" alt=\"\" border=\"0\">\n",
    "\n",
    "To fix this, we need to consider how close the keyword is to the link in question. We will update the priority function, to evaluate the priority based on distance between keyword an a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcf859-21a4-467b-81fc-cf1b9ac099b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferentialWebCrawler2(PreferentialWebCrawler):\n",
    "    def priority(self, html, link, link_tag):\n",
    "        \"\"\"\n",
    "        Compute the priority of a link.\n",
    "\n",
    "        Args:\n",
    "            html (str): HTML content of the page.\n",
    "            link (str): Link URL.\n",
    "            link_tag (bs4.Tag): BeautifulSoup tag representing the link.\n",
    "\n",
    "        Returns:\n",
    "            float: Priority score (lower number represents high priority).\n",
    "        \"\"\"\n",
    "        window_size = 50\n",
    "        # Get the content of the parent tag to the link\n",
    "        sourounding_text = link_tag.parent.text\n",
    "        index = sourounding_text.find(link_tag.text)\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(sourounding_text), index + window_size)\n",
    "        sourounding_text = sourounding_text[start:end]\n",
    "        \n",
    "        # Assign priority based on how close to the link the keyword was found (lower number -> higher priority)\n",
    "        keyword_distance = abs(sourounding_text.lower().find(self.keyword.lower()) - sourounding_text.find(link_tag.text))\n",
    "        priority = keyword_distance if self.keyword.lower() in sourounding_text.lower() else window_size*2\n",
    "\n",
    "        return priority\n",
    "\n",
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"Nobel\"  # Prioritize links containing this keyword\n",
    "crawler = PreferentialWebCrawler2(seed, keyword, max_pages=5)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f8e88",
   "metadata": {},
   "source": [
    "Now we get the nobele prize in physics, which was our goal.\n",
    "\n",
    "Let's try the example from before, where we were searching for societies where, Einstein was the president."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"president\"  # Prioritize links containing this keyword\n",
    "crawler = PreferentialWebCrawler2(seed, keyword, max_pages=5)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c6c3c",
   "metadata": {},
   "source": [
    "This time, the priority of the link, we are interested in would be >0:\n",
    "\n",
    "> <img src=\"https://i.ibb.co/N2GDkwDm/crawling.png\" alt=\"crawling\" border=\"0\">\n",
    "\n",
    "However; there are other mentions of the president keyword, that get higher priority and burry the link we want:\n",
    "\n",
    "> <img src=\"https://i.ibb.co/byZYjjn/Posnetek-zaslona-2025-03-03-ob-10-24-29.png\" alt=\"\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602b8ca",
   "metadata": {},
   "source": [
    "One way to solve the problem of buried links (links that are relevant but located deep within a website's structure) we can use an N-best first crawler instead of a best-first crawler. The approach involves balancing exploration and exploitation to ensure that potentially valuable but buried links are not overlooked.\n",
    "\n",
    "A **best-first** crawler prioritizes URLs based on their relevance score, always expanding the most promising link first, which can cause it to focus on highly relevant regions and miss deeply buried links. In contrast, an **N-best first crawler** expands the top N highest-scoring URLs in parallel, balancing exploration and exploitation to increase the likelihood of discovering relevant content across different parts of the website.\n",
    "Once the N-best links are explored, it uses merge sort to include the newly found links in the priority queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa03d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for practice, update the crawler, to support N-best first strategy and check if it finds the target link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca35681",
   "metadata": {},
   "source": [
    "### **Using Bag of Words for Priority Computation**  \n",
    "\n",
    "Another approach is to use a more descriptive representation of the target page than a simple keyword. Instead of relying on a single word, we can compare multiple words from the link’s surrounding text to a more detailed **target description**. This method allows the crawler to prioritize links whose context is more semantically similar to the desired content.\n",
    "\n",
    "In this example, we will expand the priority function to search for the phrase: **\"Society where Einstein was elected president in 1916\"**. The crawler will extract the surrounding text (neighborhood) around each link and represent it as a **Bag of Words (BoW)** vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1667aa",
   "metadata": {},
   "source": [
    "### **What are Bag of Words Vectors?**  \n",
    "\n",
    "Bag of Words is a simple yet powerful text representation method. It converts a piece of text into a vector by counting how often each word appears. The order of the words is ignored, and only the frequency matters. For example, the sentence:\n",
    "\n",
    "**\"Einstein was a famous scientist\"**  \n",
    "\n",
    "Would be represented by the vector:  \n",
    "\n",
    "| Word       | Count |\n",
    "|------------|-------|\n",
    "| Einstein   | 1     |\n",
    "| was        | 1     |\n",
    "| a          | 1     |\n",
    "| famous     | 1     |\n",
    "| scientist  | 1     |\n",
    "| some       | 0     |\n",
    "| other      | 0     |\n",
    "| words      | 0     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bag of words vector for a simple example\n",
    "vectorizer = CountVectorizer()\n",
    "texts = [\"This is one example sentence.\", \"An example sentence is a sentence that serves as an example\"]\n",
    "word_vectors = vectorizer.fit_transform(texts)\n",
    "dense_matrix = word_vectors.toarray()\n",
    "\n",
    "print(\"\\nWord Frequency Table:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    for j, word in enumerate(vectorizer.get_feature_names_out()):\n",
    "        print(f\"  {word}: {dense_matrix[i, j]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca8ed0",
   "metadata": {},
   "source": [
    "### **Computing Similarity**  \n",
    "\n",
    "To compare the **neighborhood text** with the **target description**, we will:  \n",
    "\n",
    "1. Convert both texts into Bag of Words vectors.\n",
    "2. Use **cosine similarity** to measure how similar the two vectors are.\n",
    "3. Assign higher priority to links with a higher similarity score.  \n",
    "\n",
    "Cosine similarity ranges from **0 (completely different)** to **1 (identical)**, making it a good metric for text comparison.  \n",
    "Given two vectors **A** and **B**, the **cosine similarity** is calculated as:\n",
    "\n",
    "\n",
    "$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$\n",
    "\n",
    "Where:\n",
    "- $ A \\cdot B $ is the **dot product** of vectors **A** and **B**.\n",
    "- $ \\|A\\| $ is the **magnitude** (or length) of vector **A**.\n",
    "- $ \\|B\\| $ is the **magnitude** (or length) of vector **B**.\n",
    "\n",
    "\n",
    "In the next section, we will implement the priority function using Bag of Words and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e725e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between two vectors\n",
    "similarity = cosine_similarity(word_vectors[0], word_vectors[1])[0][0]\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3304efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferentialWebCrawler3(PreferentialWebCrawler):\n",
    "    def priority(self, html, link, link_tag):\n",
    "        \"\"\"\n",
    "        Compute the priority of a link.\n",
    "\n",
    "        Args:\n",
    "            html (str): HTML content of the page.\n",
    "            link (str): Link URL.\n",
    "            link_tag (bs4.Tag): BeautifulSoup tag representing the link.\n",
    "\n",
    "        Returns:\n",
    "            float: Priority score (lower number represents high priority).\n",
    "        \"\"\"\n",
    "        window_size = 50\n",
    "        # Get the content of the parent tag to the link\n",
    "        sourounding_text = link_tag.parent.text\n",
    "        index = sourounding_text.find(link_tag.text)\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(sourounding_text), index + window_size)\n",
    "        sourounding_text = sourounding_text[start:end]\n",
    "\n",
    "        # Create Bag of Words representations\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        texts = [self.keyword, sourounding_text]\n",
    "        word_vectors = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Compute cosine similarity between the two bags of words\n",
    "        similarity = cosine_similarity(word_vectors[0], word_vectors[1])[0][0]\n",
    "        \n",
    "        # compue priority based on vector similarity (more similar texts should result in higher priority (lower return number))\n",
    "        priority = 1 - similarity\n",
    "        return priority\n",
    "\n",
    "\n",
    "seed = \"https://en.wikipedia.org/wiki/Albert_Einstein\"  # Replace with an actual URL\n",
    "keyword = \"Society where Einstein was elected president in 1916\"  # Prioritize links containing this keyword\n",
    "crawler = PreferentialWebCrawler3(seed, keyword, max_pages=5)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f162d3",
   "metadata": {},
   "source": [
    "### Advanced techniques for implementing preferential crawlers\n",
    "The goal of this notebook was to show the basic idea behind preferential crawling systems. In practice there are many ways of implementing the strategy for selecting best websites to crawl.\n",
    "\n",
    "One general method that can be combined with a variety of techniques for aproximating link relevance is to use a **Bayesian classifier** to guide the crawling process. The classifier is trained on example pages from relevant categories in the taxonomy. It predicts the probability that a newly crawled page belongs to one of the desired categories.  \n",
    "\n",
    "For each crawled page $p$, the classifier computes the probability $Pr(c|p)$ that the page belongs to category $c$. A **relevance score** is then assigned to the page:  \n",
    "\n",
    "$R(p) = \\sum_{c \\in c^*} Pr(c|p)$\n",
    "\n",
    "Where $c^*$ is the set of categories of interest selected by the user.  \n",
    "\n",
    "The crawler uses this score in two possible strategies:  \n",
    "\n",
    "- **Soft Focus Strategy**: The relevance score is used as the priority value for all outgoing links from the page. Higher scores lead to higher priority in the frontier queue.  \n",
    "- **Hard Focus Strategy**: Only links from pages classified into the desired categories or their subcategories are added to the frontier, while others are discarded.  \n",
    "\n",
    "Additionally, **distiller algorithms** can identify **topical hubs**—pages with many links to authoritative sources on the target topic. These hubs help the crawler discover high-quality content more efficiently.  \n",
    "\n",
    "By combining Bayesian classifiers with link analysis, preferential crawlers can significantly improve the quality and relevance of the pages they retrieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
